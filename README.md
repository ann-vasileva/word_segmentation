# Восстановление пробелов в предложениях
## Подход

Метод основан на вероятностной модели с использованием динамического программирования:

- В качестве корпуса используется OpenCorpora (около 1,5 млн слов, включая все формы слов). Дополнительно словарь обогащён топ-10 000 слов из объявлений Avito, чтобы включить названия товаров (например, «микроволновка»), бренды («Xbox One») и популярные слова, отсутствующие в литературном языке («бу»).

- Для значительного ускорения вычислений словарь хранится в боре. Это позволяет на каждой позиции предложения проверять не весь словарь, а только текущий префикс, что существенно сокращает количество проверок.

- В начале из текстов удаляется сложная пунктуация, которая может мешать корректному разбиению, особенно на песнях. При этом оригинальная пунктуация сохраняется, чтобы её можно было восстановить после сегментации.

- Далее текст предварительно разбивается с помощью простых эвристик: смена языка, наличие цифр, переход с строчной буквы на заглавную.

- Динамический алгоритм вычисляет вероятности для каждой позиции в предложении.

- Для подсчёта вероятностей используются частоты слов и биграммы из корпуса OpenCorpora. (В дальнейшем можно также вручную посчитать их по корпусу Avito.)

- Для каждого возможного разбиения строки алгоритм вычисляет вероятность на основе этих частот, после чего выбирается наиболее вероятное разбиение.

- После сегментации в тексте восстанавливается исходная пунктуация.

Потенциальные доработки:
- Улучшить фильтрацию слов из корпуса Avito, чтобы добавлять полезные слова, не занося случайный мусор.

- Проработать добавление пунктуации обратно, чтобы соблюдались нормы русского языка при обработке предложений.

- Доработать функцию предварительного разбиения текста эвристиками, так как она была написана до внедрения функций работы с пунктуацией.

---

## Структура репозитория
```
word_segmentation/
│
├── README.md                 
├── data/                     
│   ├── unigrams.json         # Частоты отдельных слов
│   ├── bigrams.json          # Частоты пар слов
│   └── dictionary.txt.zip    # Корпус слов русского языка
│
├──src/
│   ├── dictionary_utils.py   # Бор, построение корпуса слов
│   ├── ngram_utils.py        # Подсчет n-gram
│   ├── preprocessing.py      # Разбиение по эвристикам
│   ├── segmentation.py       # Алгоритм сегментации
│   └── utils.py              # Работа с пунктуацией 
└── pipeline.ipynb
```

## Эксперимент с нейросетью


Первоначально рассматривала использование энкодера (rubert-tiny), так как такие модели более лёгкие и эффективные для задач сегментации текста. Однако из-за ограниченности времени я решила использовать энкодер-декодер T5, поскольку для него можно было быстро подготовить тренировочный датасет.

Для датасета я брала те же описания товаров Avito с Kaggle, примерно 15 000 текстов и обрезала их по 15-20 слов.
 
Из текстов удалялись все пробелы.

Производился fine-tuning mT5-small (мультиязычный, лучше было брать rut5-small) примерно на 10 эпох.

Использовался промт "fix spaces: ", чтобы модель быстрее поняла задачу.

Результаты показали, что подход не успел достигнуть хорошего качества и обобщения:

- На тренировочных данных модель научилась разделять пробелы,

- На тестовых данных она практически не справлялась ни с русским языком, ни с задачей сегментации, выдавая несвязные тексты для большей части слов.

Для улучшения нужно было:

- брать больше данных и лучше их готовить

- либо учить модель сразу исправлять ошибки n-gram, чтобы задача была проще.

Мме не хватило времени, чтобы полностью настроить подход и добиться стабильных результатов. Постараюсь вернуться к этой задаче в будущем.
